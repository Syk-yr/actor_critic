{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DDPG_TORCS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOiKt89TnH1JGrHK/65f8Tz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Syk-yr/actor_critic/blob/main/DDPG_TORCS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvlZ8-GgRDPu",
        "outputId": "cef5ca15-93e3-4ca4-deeb-c91e3d6b7eda"
      },
      "source": [
        "import sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/DDPG_TORCS')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhcyWui8QPTB",
        "outputId": "4bbc8b11-b5c0-4802-9ef8-fe58f5321bc5"
      },
      "source": [
        "!pip install numpy"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxHfRBMEOjat"
      },
      "source": [
        "from gym_torcs import TorcsEnv\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmaNB6_0OYio"
      },
      "source": [
        "def create_env(vision):\n",
        "  return TorcsEnv(vision=vision, throttle=True,gear_change=False)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HOoN4uAPzn_",
        "outputId": "f5be370d-c7e7-42e6-cb6d-3a1624a982d8"
      },
      "source": [
        "env = create_env(False)\n",
        "num_states = env.observation_space.shape[0]\n",
        "print(num_states)\n",
        "num_actions = env.action_space.shape[0]\n",
        "print(num_actions)\n",
        "\n",
        "upper_bound = env.action_space.high\n",
        "print(upper_bound)\n",
        "low_bound = env.action_space.low\n",
        "print(low_bound)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n",
            "2\n",
            "[1. 1.]\n",
            "[-1. -1.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5JfwmPQP1He"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class OU(object):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def function(self,x,mu,theta,sigma,action_dim):\n",
        "    # x = x.reshape((action_dim,1))\n",
        "    # mu = mu.reshape((action_dim,1))\n",
        "    # theta = theta.reshape((action_dim,1))\n",
        "    # sigma =sigma.reshape((action_dim,1))\n",
        "    y = theta * (mu - x) + sigma * np.random.normal(size=(1,action_dim))\n",
        "    return y\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyPSoNK2U3hB"
      },
      "source": [
        "from collections import deque\n",
        "import random\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "  def __init__(self,buff_capacity = 100000, batch_size = 64):\n",
        "    self.buff_capacity = buff_capacity\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    self.buff_count = 0\n",
        "\n",
        "    self.buff = deque()\n",
        "  \n",
        "  def getBatch(self):\n",
        "    if self.buff_count < self.batch_size:\n",
        "      return random.sample(self.buff,self.buff_count)\n",
        "    else:\n",
        "      return random.sample(self.buff,self.batch_size)\n",
        "\n",
        "  def capacity(self):\n",
        "    return self.buff_capacity\n",
        "  \n",
        "  def add(self, state,action,reward,next_state,done):\n",
        "    exp = (state,action,reward,next_state,done)\n",
        "    if self.buff_count < self.buff_capacity:\n",
        "      self.buff.append(exp)\n",
        "      self.buff_count += 1\n",
        "    else:\n",
        "      self.buff.popleft()\n",
        "      self.buff.append(exp)\n",
        "    \n",
        "  def count(self):\n",
        "      return self.buff_count\n",
        "\n",
        "  def erase(self):\n",
        "    self.buff = deque()\n",
        "    self.buff_count = 0\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rErLi_zJX8MR"
      },
      "source": [
        "class Actor(object):\n",
        "  def __init__(self,state_size,Learning_rate):\n",
        "    self.Learning_rate = Learning_rate\n",
        "    self.state_size = state_size\n",
        "    self.model = get_actor()\n",
        "    self.target_model = get_actor()\n",
        "    self.target_model.set_weights(self.model.get_weights())\n",
        "    self.optimizer=tf.keras.optimizers.Adam(self.Learning_rate)\n",
        "\n",
        "  def get_actor(self):\n",
        "    print(\"Now we build the model\")\n",
        "    last_init = tf.random_uniform_initializer(minval=-0.05,maxval=0.05)\n",
        "    inputs = layers.Input(shape=(self.state_size,))   \n",
        "    out = layers.Dense(300, activation='relu')(inputs)\n",
        "    out = layers.Dense(600, activation='relu')(out)\n",
        "        # Steering = Dense(1,activation='tanh',init=lambda shape, name: normal(shape, scale=1e-4, name=name))(out)  \n",
        "        # Acceleration = Dense(1,activation='sigmoid',init=lambda shape, name: normal(shape, scale=1e-4, name=name))(out)   \n",
        "        # Brake = Dense(1,activation='sigmoid',init=lambda shape, name: normal(shape, scale=1e-4, name=name))(out) \n",
        "    Steering = layers.Dense(1,activation='tanh',kernel_initializer=last_init)(out)  \n",
        "    Acceleration = layers.Dense(1,activation='sigmoid',ikernel_initializer=last_init)(out)   \n",
        "    Brake = layers.Dense(1,activation='sigmoid',kernel_initializer=last_init)(out) \n",
        "    outputs = layers.Concatenate()([Steering,Acceleration,Brake])          \n",
        "    model = tf.keras.Model(input=inputs,output=outputs)\n",
        "    return model\n",
        "  \n",
        "  def get_action(self,state):\n",
        "    return tf.squeeze(self.model(state))\n",
        "    # return self.model.predict(state.reshape(1,state.shape[0]))\n",
        "    \n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joMButOKYJgl"
      },
      "source": [
        "class Critic(object):\n",
        "  def __init__(self,state_size,action_dim,Learning_rate):\n",
        "    self.Learning_rate = Learning_rate\n",
        "    self.state_size = state_size\n",
        "    self.action_dim = action_dim\n",
        "\n",
        "    self.model = self.get_critic()\n",
        "    self.target_model = self.get_critic()\n",
        "    self.target_model.set_weights(self.model.get_weights())\n",
        "    self.critic_optimizer = tf.keras.optimizers.Adam(self.Learning_rate)\n",
        "\n",
        "  def get_critic(self):\n",
        "    state_input = layers.Input(shape=(self.state_size))  \n",
        "    state_out = layers.Dense(300, activation='relu')(state_input)\n",
        "    state_out = layers.Dense(600, activation='relu')(state_out)\n",
        "\n",
        "    action_input = layers.Input(shape=(self.action_dim))   \n",
        "    action_out = layers.Dense(600, activation='relu')(action_input) \n",
        "\n",
        "    # h2 = merge([h1,a1],mode='sum')    \n",
        "    concat = layers.Concatenate()([state_out, action_out])\n",
        "    out = layers.Dense(600, activation='relu')(concat)\n",
        "    out = layers.Dense(600, activation='relu')(concat)\n",
        "    outputs = layers.Dense(1)(out)   ##################\n",
        "    model = tf.keras.Model(input=[state_input,action_input],output=outputs)\n",
        "    # adam = Adam(lr=self.LEARNING_RATE)\n",
        "    # model.compile(loss='mse', optimizer=adam)\n",
        "    return model"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUJTa_GHlJHR"
      },
      "source": [
        "class DDPG(object):\n",
        "  def __init__(self,state_dim,action_dim,Buff_capacity=100000,Batch_size = 64,Gamma=0.99,Tau=0.001,LR_A = 0.0001,LR_C = 0.001):\n",
        "    self.Buff_capacity = Buff_capacity\n",
        "    self.Batch_size = Batch_size\n",
        "    self.Gamma = Gamma\n",
        "    self.Tau = Tau\n",
        "    self.LR_A = LR_A\n",
        "    self.LR_C = LR_C\n",
        "\n",
        "    self.action_dim = action_dim #Steering/Acceleration/Brake\n",
        "    self.state_dim = state_dim # 29\n",
        "    np.random.seed(1337)\n",
        "    self.buff = ReplayBuffer(self.Buff_capacity,self.Batch_size)\n",
        "    self.actor = Actor(self.state_dim,self.LR_A)\n",
        "    self.critic = Critic(self.state_dim,self.action_dim,self.LR_C)\n",
        "          \n",
        "\n",
        "  def update(self,state_batch,action_batch,reward_batch,next_state_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "      actions = self.actor.model(state_batch,training=True)\n",
        "      critic_value = self.critic.model([state_batch,actions],training=True)\n",
        "      actor_loss = -tf.math.reduce_mean(critic_value)\n",
        "    \n",
        "    actor_grad = tape.gradient(actor_loss,self.actor_model.trainable_variables)\n",
        "    self.actor.optimizer.apply_gradients(zip(actor_grad,self.actor.model.trainable_variables))\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      target_actions = self.actor.target_model(next_state_batch,training=True)\n",
        "      y = reward_batch + self.Gamma * self.critic.target_model([next_state_batch,target_actions],training=True)\n",
        "      critic_value = self.critic.model([state_batch,action_batch],traing=True)\n",
        "      critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
        "      actor_loss = -tf.math.reduce_mean(critic_value)\n",
        "    \n",
        "    actor_grad = tape.gradient(actor_loss,self.actor_model.trainable_variables)\n",
        "    self.actor.optimizer.apply_gradients(zip(actor_grad,self.actor.model.trainable_variables))\n",
        "\n",
        "  def update_target(self,target_weights,weights):\n",
        "    for (a,b) in zip(target_weights,weights):\n",
        "      a.assign(b*self.Tau + a * (1 - self.Tau))\n",
        "\n",
        "  def learn(self):\n",
        "    batch = self.buff.getBatch()\n",
        "    state_batch = tf.convert_to_tensor([e[0] for e in batch])\n",
        "    action_batch = tf.convert_to_tensor([e[1] for e in batch])\n",
        "    reward_batch = tf.convert_to_tensor([e[2] for e in batch])\n",
        "    reward_batch = tf.cast(reward_batch,dtype=tf.float32)\n",
        "    next_state_batch = tf.convert_to_tensor([e[3] for e in batch])\n",
        "    done_batch = np.asarray([e[4] for e in batch])\n",
        "\n",
        "    self.update(state_batch,action_batch,reward_batch,next_state_batch)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0pkWJVhnINq"
      },
      "source": [
        "import json\n",
        "def playGame(train_indicator = 0):\n",
        "  BUFFER_CAPACITY = 100000\n",
        "  BATCH_SIZE = 32\n",
        "  GAMMA = 0.99\n",
        "  TAU = 0.001     #Target Network HyperParameters\n",
        "  LRA = 0.0001    #Learning rate for Actor\n",
        "  LRC = 0.001     #Lerning rate for Critic\n",
        "  EXPLORE = 1000000.\n",
        "  action_dim = 3  #Steering/Acceleration/Brake\n",
        "  state_dim = 29  #of sensors input\n",
        "\n",
        "  np.random.seed(1337)\n",
        "  ep_reward_list = []\n",
        "  avg_reward_list = []\n",
        "\n",
        "  vision = False\n",
        "  episode_count = 2000\n",
        "  max_steps = 100000\n",
        "  reward = 0\n",
        "  done = False\n",
        "  step = 0\n",
        "  epsilon = 1\n",
        "  indicator = 0\n",
        "  mu = np.array([0.0,0.5,-0.1])#计算动作噪声时使用\n",
        "  theta = np.array([0.60,1.00,1.00])\n",
        "  sigma = np.array([0.30,0.10,0.05])\n",
        "\n",
        "  ddpg = DDPG(BUFFER_CAPACITY,BATCH_SIZE,GAMMA,TAU,LRA,LRC)\n",
        "  ou = OU()\n",
        "\n",
        "  env = create_env(False)\n",
        "  #Now load the weight\n",
        "  print(\"Now we load the weight\")\n",
        "  try:\n",
        "    ddpg.actor.model.load_weights(\"actormodel.h5\")\n",
        "    ddpg.critic.model.load_weights(\"criticmodel.h5\")\n",
        "    ddpg.actor.target_model.load_weights(\"actormodel.h5\")  \n",
        "    ddpg.critic.target_model.load_weights(\"criticmodel.h5\")    \n",
        "    print(\"Weight load successfully\")\n",
        "  except:\n",
        "    print(\"Cannot find the weight\")\n",
        "  \n",
        "  print(\"TORCS Experiment Start.\")\n",
        "\n",
        "  for ep in range(episode_count):\n",
        "    if np.mod(ep,3) == 0:\n",
        "      ob = env.reset(relaunch=True)\n",
        "    else:\n",
        "      ob = env.reset()\n",
        "    \n",
        "    state = np.hstack((ob.angle, ob.track, ob.trackPos, ob.speedX, ob.speedY,  ob.speedZ, ob.wheelSpinVel/100.0, ob.rpm))\n",
        "    episodic_reward = 0.\n",
        "    for step in range(max_steps):\n",
        "      loss = 0\n",
        "      epsilon -= 1.0/EXPLORE\n",
        "      action = ddpg.actor.get_action(state)\n",
        "      noise = train_indicator * max(epsilon,0) * ou.function(action,mu,theta,sigma,action_dim)\n",
        "      action = action + noise\n",
        "\n",
        "      ob,reward,done,info = env.step(action[0])\n",
        "      next_state = np.hstack((ob.angle, ob.track, ob.trackPos, ob.speedX, ob.speedY, ob.speedZ, ob.wheelSpinVel/100.0, ob.rpm))\n",
        "      ddpg.buff.add(state,action,reward,next_state,done)\n",
        "      episodic_reward += reward\n",
        "\n",
        "      if train_indicator:\n",
        "        ddpg.learn()\n",
        "        ddpg.update_taeget(ddpg.actor.target_model.variables,ddpg.actor.model.variables)\n",
        "        ddpg.update_taeget(ddpg.critic.target_model.variables,ddpg.critic.model.variables)\n",
        "\n",
        "      state = next_state\n",
        "\n",
        "      if step % 1000 == 0:\n",
        "        print(\"Episode:{} step:{} action:{} Reward:{}\".format(ep, step,action,reward))\n",
        "\n",
        "      if done:\n",
        "        break\n",
        "      \n",
        "    if np.mod(ep,3) == 0:\n",
        "      if train_indicator:\n",
        "        print(\"Now we save model\")\n",
        "        ddpg.actor.model.save_weights(\"actormodel.h5\", overwrite=True)\n",
        "        with open(\"actormodel.json\", \"w\") as outfile:\n",
        "          json.dump(ddpg.actor.model.to_json(), outfile)\n",
        "\n",
        "        ddpg.critic.model.save_weights(\"criticmodel.h5\", overwrite=True)\n",
        "        with open(\"actormodel.json\", \"w\") as outfile:\n",
        "          json.dump(ddpg.actor.model.to_json(), outfile)\n",
        "    \n",
        "    ep_reward_list.append(episodic_reward)\n",
        "    # Mean of last 40 episodes\n",
        "    avg_reward = np.mean(ep_reward_list[-40:])\n",
        "    avg_reward_list.append(avg_reward)\n",
        "    print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
        "  \n",
        "  env.end()  # This is for shutting down TORCS\n",
        "  print(\"Finish.\")\n",
        "\n",
        "  \n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiAWMsBHjDNI"
      },
      "source": [
        "def function(x,mu,theta,sigma,action_dim):\n",
        "  # x = x.reshape((action_dim,1))\n",
        "  # mu = mu.reshape((action_dim,1))\n",
        "  # theta = theta.reshape((action_dim,1))\n",
        "  # sigma =sigma.reshape((action_dim,1))\n",
        "  y = theta * (mu - x) + sigma * np.random.normal(size=(1,action_dim))\n",
        "  return y"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzrwOse8jF-L",
        "outputId": "0dbebb13-cd57-4b7c-f7ab-7ecb18529068"
      },
      "source": [
        "mu = np.array([0.0,0.5,-0.1])#计算动作噪声时使用\n",
        "theta = np.array([0.60,1.00,1.00])\n",
        "sigma = np.array([0.30,0.10,0.05])\n",
        "x = np.array([1.,2.,3.])\n",
        "y = function(x,mu,theta,sigma,3)\n",
        "x = x +y\n",
        "print(x)\n",
        "print(x[0])\n",
        "print(y)\n",
        "print(y.shape)\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.28765463  0.41551065 -0.05315897]]\n",
            "[ 0.28765463  0.41551065 -0.05315897]\n",
            "[[-0.71234537 -1.58448935 -3.05315897]]\n",
            "(1, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiWST-ETjSi5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}